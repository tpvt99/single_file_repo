{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding Attention\n",
    "\n",
    "This notebook will help us to understand Bahdanu and Luong Attention"
   ],
   "metadata": {
    "id": "DDilIPeouuJ6",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For Google Colaboratory\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Running as a Jupyter notebook - intended for development only!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OK0V5flfdf1",
    "outputId": "e61dfd90-99ce-4013-96bc-5ffa7d8dbca9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import einops\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from torchinfo import summary\n",
    "from torchview import draw_graph\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Downloading data for attention\n",
    "if not os.path.exists('data/attention/dataset'):\n",
    "    !gdown 1ZNJ5sVf5FtdbR7xlbw3ybfiZVd_yXI0p\n",
    "    os.makedirs('data/attention', exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data/attention')\n",
    "        os.remove('dataset.zip')\n",
    "\n",
    "DATA_PATH = 'data/attention/dataset/'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.4.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from fr-core-news-sm==3.4.0) (3.4.4)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.23.4)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.10.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\r\n",
      "Requirement already satisfied: setuptools in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (61.2.0)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (22.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.6)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.28.1)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (6.3.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.1.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.4.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.26.13)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.12.7)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.1.1)\r\n",
      "\u001B[33mWARNING: There was an error checking the latest version of pip.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n",
      "Collecting en-core-web-sm==3.4.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\r\n",
      "Requirement already satisfied: jinja2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (22.0)\r\n",
      "Requirement already satisfied: setuptools in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.4)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.13)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\r\n",
      "\u001B[33mWARNING: There was an error checking the latest version of pip.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "# Downloading english and french tokenizer\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Extract dataset"
   ],
   "metadata": {
    "id": "Hm8iGWGngefH",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# For this dataset, we are trying to translate french to english\n",
    "SRC_LANGUAGE = 'fr'\n",
    "TGT_LANGUAGE = 'en'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# First, we create a custom dataset to load the data. Each item is a pair of french and english datapoint\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, train_size=10000, test_size=1000, max_len=50):\n",
    "        self.en_dir = os.path.join(\"data/attention/dataset\", \"europarl-v7.fr-en.en\")\n",
    "        self.fr_dir = os.path.join(\"data/attention/dataset\", \"europarl-v7.fr-en.fr\")\n",
    "        with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "            self.english_data = f.readlines()\n",
    "        with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "            self.french_data = f.readlines()\n",
    "        # Only train on sentences with less than 50 letters\n",
    "        self.indicies = np.array([i for i in range(len(self.english_data)) if len(self.english_data[i]) < max_len])\n",
    "        # First 10000 datapoints for train\n",
    "        if train:\n",
    "          with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "              self.english_data = [self.english_data[i] for i in self.indicies][:train_size]\n",
    "          with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "              self.french_data = [self.french_data[i] for i in self.indicies][:train_size]\n",
    "        # Next 10000 datatpoints for test\n",
    "        else:\n",
    "            with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "                self.english_data = [self.english_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
    "            with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "                self.french_data = [self.french_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
    "                # self.french_data = f.readlines()[train_size:train_size+test_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.french_data[idx], self.english_data[idx]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Instantiate dataset\n",
    "dataset = CustomDataset(train=True)\n",
    "\n",
    "# Next, we load the tokenizer that transforms the input sentence into tokens\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Helper function to generate list[list[token]], where each inner list: list[token] is a list of token per sentence.\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reprise de la session\\n', 'Resumption of the session\\n')\n",
      "----\n",
      "[['Reprise', 'de', 'la', 'session', '\\n']]\n",
      "[['Resumption', 'of', 'the', 'session', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "# For demonstration of yield_tokens() function\n",
    "for z in iter(dataset):\n",
    "    print(z) # print out each item in dataset, 0 index contains source language, 1 index contains target language\n",
    "    print('----')\n",
    "    print(list(yield_tokens([z], SRC_LANGUAGE))) # Token of source langauge for this item\n",
    "    print(list(yield_tokens([z], TGT_LANGUAGE))) # Token of source langauge for this item\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Next, we build the dictionary to convert the tokens to indicies.\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "vocab_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = iter(dataset)\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Functions transform the input sentence to a format that can be used for training\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Adding BOS, EOS and then make tensor\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# A function does everything and also padding\n",
    "def collate_fn(src, tgt):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in zip(src, tgt):\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Print an example\n",
    "batch_size = 2\n",
    "dataset = CustomDataset(train=True)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reprise de la session\\n', 'Resumption of the session\\n')\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "print(next(iter(dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Inputs: (\"Je ne pense donc pas qu'il faudrait aller jusque là.\\n\", 'Le débat est clos.\\n')\n",
      "('So I do not think we should go that far.\\n', 'The debate is closed.\\n')\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "fr_sentence, eng_sentence = next(iter(train_dataloader))\n",
    "print(f\"Raw Inputs: {fr_sentence}\\n{eng_sentence}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs: [['Je', 'ne', 'pense', 'donc', 'pas', \"qu'\", 'il', 'faudrait', 'aller', 'jusque', 'là', '.'], ['Le', 'débat', 'est', 'clos', '.']]\n",
      "[['So', 'I', 'do', 'not', 'think', 'we', 'should', 'go', 'that', 'far', '.'], ['The', 'debate', 'is', 'closed', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Token transform\n",
    "fr_token, eng_token = [token_transform[\"fr\"](i.rstrip(\"\\n\")) for i in fr_sentence], [token_transform[\"en\"](i.rstrip(\"\\n\")) for i in eng_sentence]\n",
    "print(f\"Tokenized Inputs: {fr_token}\\n{eng_token}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs to indicies: [[16, 24, 120, 80, 17, 72, 47, 699, 426, 6276, 124, 5], [12, 42, 6, 52, 5]]\n",
      "[[86, 10, 41, 16, 89, 28, 62, 183, 22, 199, 5], [11, 37, 6, 42, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Vocab transform\n",
    "fr_idx, eng_idx = [vocab_transform[\"fr\"](i) for i in fr_token], [vocab_transform[\"en\"](i) for i in eng_token]\n",
    "print(f\"Tokenized Inputs to indicies: {fr_idx}\\n{eng_idx}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Indicies with begin (2) and end token (3): [tensor([   2,   16,   24,  120,   80,   17,   72,   47,  699,  426, 6276,  124,\n",
      "           5,    3]), tensor([ 2, 12, 42,  6, 52,  5,  3])]\n",
      "[tensor([  2,  86,  10,  41,  16,  89,  28,  62, 183,  22, 199,   5,   3]), tensor([ 2, 11, 37,  6, 42,  5,  3])]\n"
     ]
    }
   ],
   "source": [
    "# Tensor transform\n",
    "fr_pad, eng_pad = [tensor_transform(i) for i in fr_idx], [tensor_transform(i) for i in eng_idx]\n",
    "print(f\"Tokenized Indicies with begin (2) and end token (3): {fr_pad}\\n{eng_pad}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding (1): tensor([[   2,    2],\n",
      "        [  16,   12],\n",
      "        [  24,   42],\n",
      "        [ 120,    6],\n",
      "        [  80,   52],\n",
      "        [  17,    5],\n",
      "        [  72,    3],\n",
      "        [  47,    1],\n",
      "        [ 699,    1],\n",
      "        [ 426,    1],\n",
      "        [6276,    1],\n",
      "        [ 124,    1],\n",
      "        [   5,    1],\n",
      "        [   3,    1]])\n",
      "tensor([[  2,   2],\n",
      "        [ 86,  11],\n",
      "        [ 10,  37],\n",
      "        [ 41,   6],\n",
      "        [ 16,  42],\n",
      "        [ 89,   5],\n",
      "        [ 28,   3],\n",
      "        [ 62,   1],\n",
      "        [183,   1],\n",
      "        [ 22,   1],\n",
      "        [199,   1],\n",
      "        [  5,   1],\n",
      "        [  3,   1]])\n"
     ]
    }
   ],
   "source": [
    "# # Lastly, we pad the rest of the sentence\n",
    "# This also changes the shape from (bs, seq_len) to (seq_len, bs)\n",
    "fr_pad, eng_pad = pad_sequence(fr_pad, padding_value=PAD_IDX), pad_sequence(eng_pad, padding_value=PAD_IDX)\n",
    "print(f\"After padding (1): {fr_pad}\\n{eng_pad}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Outputs: tensor([[   2,    2],\n",
      "        [  16,   12],\n",
      "        [  24,   42],\n",
      "        [ 120,    6],\n",
      "        [  80,   52],\n",
      "        [  17,    5],\n",
      "        [  72,    3],\n",
      "        [  47,    1],\n",
      "        [ 699,    1],\n",
      "        [ 426,    1],\n",
      "        [6276,    1],\n",
      "        [ 124,    1],\n",
      "        [   5,    1],\n",
      "        [   3,    1]])\n",
      "tensor([[  2,   2],\n",
      "        [ 86,  11],\n",
      "        [ 10,  37],\n",
      "        [ 41,   6],\n",
      "        [ 16,  42],\n",
      "        [ 89,   5],\n",
      "        [ 28,   3],\n",
      "        [ 62,   1],\n",
      "        [183,   1],\n",
      "        [ 22,   1],\n",
      "        [199,   1],\n",
      "        [  5,   1],\n",
      "        [  3,   1]])\n"
     ]
    }
   ],
   "source": [
    "# All the above is combined into collate_fn\n",
    "x, y = collate_fn(fr_sentence, eng_sentence)\n",
    "print(f\"Same Outputs: {x}\\n{y}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Luong Attention\n",
    "\n",
    "Implement GRU-based seq2seq model with Luong attention (https://arxiv.org/pdf/1508.04025.pdf) and train the model on the french to english translation dataset. (the use of the function nn.GRU() is allowed but the attention scheme needs to be implemented **explicitly**) :\n",
    "\n",
    "The Luong attention algorithm performs the following operations:\n",
    "\n",
    "1. The encoder generates a set of hidden states, $H = \\textbf{h}_i, i = 1, 2, .....T$ , from the input sentence. The decoder generates a set of hidden states, $S = \\textbf{s}_t, t =1, 2, .....$.\n",
    "2. The current decoder hidden state is computed as: $\\textbf{s}_t = GRU_{decoder}(\\textbf{s}_{t-1}, y_{t-1})$. Here, $\\textbf{s}_{t-1}$ denotes the previous hidden decoder state, and $y_{t-1}$ the current input, which is also the expected output for the previous timestep.\n",
    "\n",
    "3. A dot product on the encoder hidden state $\\textbf{h}_i$ and the current decoder hidden state $\\textbf{s}_t$ to compute the alignment scores: $e_{t,i} = \\textbf{s}_t . \\textbf{h}_i$. \n",
    "\n",
    "4. A softmax function is applied to the alignment scores, effectively normalizing them into attention weights in a range between 0 and 1: $\\alpha_{t, i} = \\text{softmax}(e_{t, i}/ \\textbf{e}_t)$.\n",
    "\n",
    "5. These attention weights together with the encoder hidden states are used to generate a context vector through a weighted sum: $\\textbf{c}_t = \\sum_{i=1}^T\\alpha_{t, i}\\textbf{h}_i$.\n",
    "\n",
    "6. An attentional hidden state is computed based on a weighted concatenation of the context vector and the current decoder hidden state: $\\tilde{\\textbf{s}_t} = \\text{tanh}\\big(W_c\\big[\\textbf{c}_t; \\textbf{s}_t\\big]\\big)$.\n",
    "\n",
    "7. The decoder produces a final output by feeding it a weighted attentional hidden state: $y_t = \\text{softmax}(W_y\\tilde{\\textbf{s}_t})$.\n",
    "\n",
    "8. Steps 2-7 are repeated until the end of the sequence. \n",
    "\n",
    "The attention has to be calculated in parallel via matrix multiplication. For loop $\\textbf{should not}$ be used.\n"
   ],
   "metadata": {
    "id": "2F_kTNpUg_tP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Fill UP ATTENTION NETWORK\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "\n",
    "        # for encoder\n",
    "        self.encoder_embedding = nn.Embedding(num_embeddings=SRC_VOCAB_SIZE, embedding_dim=hidden_size)\n",
    "        self.gru_encoder = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        # for decoder\n",
    "        self.decoder_embedding = nn.Embedding(num_embeddings=TGT_VOCAB_SIZE, embedding_dim=hidden_size)\n",
    "        self.gru_decoder = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        self.weighted_concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, TGT_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, h_init):\n",
    "        # x shape (seq_length, batch_size)\n",
    "        # y shape (seq_length, batch_size)\n",
    "        # h_init (1, batch_size, hidden_dim)\n",
    "\n",
    "        # encoder\n",
    "        # Shape (seq, batch, hidden_dim)\n",
    "        encoder_embedded = self.encoder_embedding(x)\n",
    "        # encoder_outputs (seq, batch_size, hidden_dim)\n",
    "        # encoder_hidden (batch_size, hidden_dim)\n",
    "        encoder_outputs, encoder_hidden = self.gru_encoder(encoder_embedded, h_init)\n",
    "\n",
    "        # decoder\n",
    "        # for now sequential:\n",
    "        # Shape (batch_size, hidden_dim)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # shape (seq_len, batch_size, hidden_dim)\n",
    "        decoder_embedded = self.decoder_embedding(y)\n",
    "        # decoder_outputs (seq, batch_size, hidden_dim)\n",
    "        # decoder_hidden (batch_size, hidden_dim)\n",
    "        decoder_outputs, decoder_hidden = self.gru_decoder(decoder_embedded, decoder_hidden)\n",
    "\n",
    "        # alignment score using dot product\n",
    "        # For each of the decoder hidden seq_out_[s], whose dimensions is [i], we dot product with\n",
    "        # every encoder hidden seq_in_[x], whose dimensions is [i], so that it is a scalar number for\n",
    "        # each [s,x]. Here we use einsum: sbi means the (seq_out_size, batch, dims),\n",
    "        # xbi means (seq_in_size, batch, dims), and we dot product of the last dimensions so in total\n",
    "        # the results is size_out, batch, size_in\n",
    "\n",
    "        # Shape (seq_out, batch, seq_in)\n",
    "        alignment_score = torch.einsum('sbi,xbi->sbx', [decoder_outputs, encoder_outputs])\n",
    "\n",
    "        #Shape (seq_out, batch, seq_in), if we sum over seq_in, we see probabilities = 1\n",
    "        # this mean for each seq_out, which token in seq_in to attend to\n",
    "        attention_weights = torch.nn.functional.softmax(alignment_score, dim=-1)\n",
    "\n",
    "        # The context is just the weighted of encoder_outputs by the attention_weights,\n",
    "        # attention_weights (seq_out, batch, seq_in), for each seq_out_[s] whose\n",
    "        # seq_in_[x] (a constant) contains probability, we multiply with encoder_outputs dimesions at each\n",
    "        # seq_in_[x] and summing together to acehive weighted encoder_outputs dim\n",
    "\n",
    "        # Shape (batch, seq_out, dim) # This calculates weights of each token in seq_out\n",
    "        context = torch.einsum('sbx,xbj->sbj', [attention_weights, encoder_outputs])\n",
    "\n",
    "        # Shape (seq_out, batch, dim*2)\n",
    "        concat_features = torch.cat([decoder_outputs, context], dim=-1)\n",
    "\n",
    "        # Shape (seq_out, batch, dim)\n",
    "        s_tilded = torch.tanh(self.weighted_concat(concat_features))\n",
    "\n",
    "        # Shape (seq_out, batch, VOCAB_SIZE)\n",
    "        output = self.out(s_tilded) # Swap batch size to first\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m hidden_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n\u001B[1;32m      6\u001B[0m net \u001B[38;5;241m=\u001B[39m LuongAttention(hidden_dim)\n\u001B[0;32m----> 8\u001B[0m summary(\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, input_size\u001B[38;5;241m=\u001B[39m [(seq_length_in, batch_size), (seq_length_out, batch_size),\n\u001B[1;32m      9\u001B[0m                           (\u001B[38;5;241m1\u001B[39m, batch_size, hidden_dim)],\n\u001B[1;32m     10\u001B[0m         dtypes\u001B[38;5;241m=\u001B[39m[torch\u001B[38;5;241m.\u001B[39mint, torch\u001B[38;5;241m.\u001B[39mint, torch\u001B[38;5;241m.\u001B[39mfloat])\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/module.py:989\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    985\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    986\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 989\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/rnn.py:194\u001B[0m, in \u001B[0;36mRNNBase._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;28;01mlambda\u001B[39;00m wn: \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, wn) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, wn) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)(wn) \u001B[38;5;28;01mfor\u001B[39;00m wn \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights_names]\n\u001B[1;32m    193\u001B[0m \u001B[38;5;66;03m# Flattens params (on CUDA)\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/rnn.py:180\u001B[0m, in \u001B[0;36mRNNBase.flatten_parameters\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    179\u001B[0m     num_weights \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 180\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cudnn_rnn_flatten_weight\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cudnn_mode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "# We use torchinfo to visualize the LuongAttention networks\n",
    "batch_size = 8\n",
    "seq_length_in = 10\n",
    "seq_length_out = 15\n",
    "hidden_dim = 32\n",
    "net = LuongAttention(hidden_dim)\n",
    "\n",
    "summary(net.to(device), input_size= [(seq_length_in, batch_size), (seq_length_out, batch_size),\n",
    "                          (1, batch_size, hidden_dim)],\n",
    "        dtypes=[torch.int, torch.int, torch.float])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache ()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model_graph \u001B[38;5;241m=\u001B[39m draw_graph(\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m      2\u001B[0m                          input_size\u001B[38;5;241m=\u001B[39m [(seq_length_in, batch_size), (seq_length_out, batch_size),\n\u001B[1;32m      3\u001B[0m                           (\u001B[38;5;241m1\u001B[39m, batch_size, hidden_dim)],\n\u001B[1;32m      4\u001B[0m         dtypes\u001B[38;5;241m=\u001B[39m[torch\u001B[38;5;241m.\u001B[39mint, torch\u001B[38;5;241m.\u001B[39mint, torch\u001B[38;5;241m.\u001B[39mfloat], roll\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, expand_nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      5\u001B[0m                          hide_module_functions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, hide_inner_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m model_graph\u001B[38;5;241m.\u001B[39mvisual_graph\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/module.py:989\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    985\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    986\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 989\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/rnn.py:194\u001B[0m, in \u001B[0;36mRNNBase._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;28;01mlambda\u001B[39;00m wn: \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, wn) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, wn) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)(wn) \u001B[38;5;28;01mfor\u001B[39;00m wn \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights_names]\n\u001B[1;32m    193\u001B[0m \u001B[38;5;66;03m# Flattens params (on CUDA)\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/anaconda3/envs/sota/lib/python3.9/site-packages/torch/nn/modules/rnn.py:180\u001B[0m, in \u001B[0;36mRNNBase.flatten_parameters\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    179\u001B[0m     num_weights \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 180\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cudnn_rnn_flatten_weight\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cudnn_mode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "model_graph = draw_graph(net.to(device),\n",
    "                         input_size= [(seq_length_in, batch_size), (seq_length_out, batch_size),\n",
    "                          (1, batch_size, hidden_dim)],\n",
    "        dtypes=[torch.int, torch.int, torch.float], roll=True, expand_nested=True,\n",
    "                         hide_module_functions=False, hide_inner_tensors=False)\n",
    "model_graph.visual_graph\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "'./temp_roll.pdf'"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_graph.visual_graph.render('temp_roll')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "hidden_size = 256\n",
    "my_lr = 1.3\n",
    "bs = 32\n",
    "\n",
    "# Variables\n",
    "train_dataset = CustomDataset(train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = CustomDataset(train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "net = LuongAttention(hidden_size)\n",
    "\n",
    "print(net)\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
   ],
   "metadata": {
    "id": "61CLe7y-x9P7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3095678a-2ab5-4da1-f601-4bfc00296d30",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    h=h.to(device)\n",
    "\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = collate_fn(x, y)\n",
    "        # Batch size might be different for the last batch\n",
    "        batch_size = x.size()[1]\n",
    "        seq_length = y.size()[0] - 1\n",
    "        # set the initial h to be the zero vector\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        # send them to the gpu\n",
    "        minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "        minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "        h=h.to(device)\n",
    "\n",
    "        # FILL UP FORWARD PASS\n",
    "        scores = net(minibatch_data, minibatch_label, h)\n",
    "        #minibatch_label = torch.swapaxes(minibatch_label, 0, 1)\n",
    "        #scores = scores[:, :-1, :] #\n",
    "        minibatch_label = minibatch_label[:, 1:]\n",
    "        loss = criterion(torch.swapaxes(scores, 1, 2), minibatch_label)\n",
    "        # END OF FORWARD PASS\n",
    "\n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # update the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        # Collect garbage to prevent OOM\n",
    "        gc.collect()\n",
    "\n",
    "    total_loss = running_loss/num_batches\n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    return math.exp(total_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:\n",
    "        for p in net.parameters():\n",
    "            p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def train(my_lr):\n",
    "    for epoch in range(num_epochs):\n",
    "      # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "        if epoch >= 4:\n",
    "            my_lr = my_lr / 1.1\n",
    "\n",
    "        # create a new optimizer and give the current learning rate.\n",
    "        optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "\n",
    "        # set the running quantities to zero at the beginning of the epoch\n",
    "        running_loss=0\n",
    "        num_batches=0\n",
    "\n",
    "        for x, y in train_dataloader:\n",
    "            # Set the gradients to zeros\n",
    "            optimizer.zero_grad()\n",
    "            # Transform inputs\n",
    "            x, y = collate_fn(x, y)\n",
    "            # Batch size might be different for the last batch\n",
    "            batch_size = x.size()[1]\n",
    "            seq_length = y.size()[0] - 1\n",
    "            # set the initial h to be the zero vector\n",
    "            h = torch.zeros(1, batch_size, hidden_size)\n",
    "            # send them to the gpu\n",
    "            minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "            minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "            h=h.to(device)\n",
    "\n",
    "            # FILL UP FORWARD PASS\n",
    "            scores = net(minibatch_data, minibatch_label, h) # Output is (seq, bs, dims)\n",
    "            # Label must have shape (batch_size, seq)\n",
    "            minibatch_label = torch.einsum('sb->bs', [minibatch_label])[:, 1:]\n",
    "            # Scores must have shape (batch, dims, seq)\n",
    "            # In torch document, input shape (N,C,d_1,...,d_k), where C is class which means\n",
    "            # dims is class because it outputs the probabilities of vocab size\n",
    "            scores = torch.einsum('sbd->bds', [scores])[:, :, :-1]\n",
    "            loss = criterion(scores, minibatch_label)\n",
    "            # END OF FORWARD PASS\n",
    "\n",
    "            # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "            loss.backward()\n",
    "\n",
    "            # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "            normalize_gradient(net)\n",
    "            optimizer.step()\n",
    "            # update the running loss\n",
    "            running_loss += loss.detach().item()\n",
    "            num_batches += 1\n",
    "            # Collect garbage to prevent OOM\n",
    "            gc.collect()\n",
    "        # compute stats for the full training set\n",
    "        total_loss = running_loss / num_batches\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print('')\n",
    "        print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "        #eval_on_test_set()\n",
    "\n",
    "train(my_lr=my_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "count = 0\n",
    "for x, y in test_dataloader:\n",
    "    print(x)\n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    x, y = collate_fn(x, y)\n",
    "    # send them to the gpu\n",
    "    minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "    # The first prediction is the start of sentence index\n",
    "    start_index = torch.tensor([[2]]).type(torch.LongTensor).to(device)\n",
    "    predictions=start_index\n",
    "    for _ in range(20):\n",
    "      # At every loop, pass in the previous predictions\n",
    "      predictions = net.forward(minibatch_data, predictions, h)\n",
    "      predictions = torch.reshape(predictions, (-1, TGT_VOCAB_SIZE, 1))\n",
    "      # Get the new predictions shifted right by 1 timestep\n",
    "      predictions = torch.argmax(predictions, dim=1)\n",
    "      # Add back the first timestep\n",
    "      predictions = torch.cat([start_index, predictions], 0)\n",
    "      if predictions[-1].item() == 3:\n",
    "          break\n",
    "    predictions = predictions.reshape(-1)\n",
    "    # Transform from token to words\n",
    "    predictions = [vocab_transform[TGT_LANGUAGE].lookup_token(i) for i in predictions]\n",
    "    print(f\"Label: {[vocab_transform[TGT_LANGUAGE].lookup_token(i) for i in y]}\")\n",
    "    print(f\"Predicted: {predictions}\")\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ],
   "metadata": {
    "id": "RijG2UdmSTdT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b1f1d22-62bb-46fe-fa45-7dc7769a7928",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MWjgSSE1wBOf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}