{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding Attention\n",
    "\n",
    "This notebook will help us to understand Bahdanu and Luong Attention"
   ],
   "metadata": {
    "id": "DDilIPeouuJ6",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For Google Colaboratory\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Running as a Jupyter notebook - intended for development only!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OK0V5flfdf1",
    "outputId": "e61dfd90-99ce-4013-96bc-5ffa7d8dbca9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import einops\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from torchinfo import summary\n",
    "from torchview import draw_graph\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Downloading data for attention\n",
    "if not os.path.exists('data/attention/dataset'):\n",
    "    !gdown 1ZNJ5sVf5FtdbR7xlbw3ybfiZVd_yXI0p\n",
    "    os.makedirs('data/attention', exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data/attention')\n",
    "        os.remove('dataset.zip')\n",
    "\n",
    "DATA_PATH = 'data/attention/dataset/'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.4.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from fr-core-news-sm==3.4.0) (3.4.4)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.23.4)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.10.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\r\n",
      "Requirement already satisfied: setuptools in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (61.2.0)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (22.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.6)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.28.1)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (6.3.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.1.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.4.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.26.13)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.12.7)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.1.1)\r\n",
      "\u001B[33mWARNING: There was an error checking the latest version of pip.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n",
      "Collecting en-core-web-sm==3.4.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\r\n",
      "Requirement already satisfied: jinja2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (22.0)\r\n",
      "Requirement already satisfied: setuptools in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.4)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.13)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bptran/anaconda3/envs/sota/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\r\n",
      "\u001B[33mWARNING: There was an error checking the latest version of pip.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "# Downloading english and french tokenizer\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Extract dataset"
   ],
   "metadata": {
    "id": "Hm8iGWGngefH",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# For this dataset, we are trying to translate french to english\n",
    "SRC_LANGUAGE = 'fr'\n",
    "TGT_LANGUAGE = 'en'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# First, we create a custom dataset to load the data. Each item is a pair of french and english datapoint\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, train_size=10000, test_size=1000, max_len=50):\n",
    "        self.en_dir = os.path.join(\"data/attention/dataset\", \"europarl-v7.fr-en.en\")\n",
    "        self.fr_dir = os.path.join(\"data/attention/dataset\", \"europarl-v7.fr-en.fr\")\n",
    "        with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "            self.english_data = f.readlines()\n",
    "        with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "            self.french_data = f.readlines()\n",
    "        # Only train on sentences with less than 50 letters\n",
    "        self.indicies = np.array([i for i in range(len(self.english_data)) if len(self.english_data[i]) < max_len])\n",
    "        # First 10000 datapoints for train\n",
    "        if train:\n",
    "          with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "              self.english_data = [self.english_data[i] for i in self.indicies][:train_size]\n",
    "          with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "              self.french_data = [self.french_data[i] for i in self.indicies][:train_size]\n",
    "        # Next 10000 datatpoints for test\n",
    "        else:\n",
    "            with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "                self.english_data = [self.english_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
    "            with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "                self.french_data = [self.french_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
    "                # self.french_data = f.readlines()[train_size:train_size+test_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.french_data[idx], self.english_data[idx]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Instantiate dataset\n",
    "dataset = CustomDataset(train=True)\n",
    "\n",
    "# Next, we load the tokenizer that transforms the input sentence into tokens\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Helper function to generate list[list[token]], where each inner list: list[token] is a list of token per sentence.\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reprise de la session\\n', 'Resumption of the session\\n')\n",
      "----\n",
      "[['Reprise', 'de', 'la', 'session', '\\n']]\n",
      "[['Resumption', 'of', 'the', 'session', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "# For demonstration of yield_tokens() function\n",
    "for z in iter(dataset):\n",
    "    print(z) # print out each item in dataset, 0 index contains source language, 1 index contains target language\n",
    "    print('----')\n",
    "    print(list(yield_tokens([z], SRC_LANGUAGE))) # Token of source langauge for this item\n",
    "    print(list(yield_tokens([z], TGT_LANGUAGE))) # Token of source langauge for this item\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Next, we build the dictionary to convert the tokens to indicies.\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "vocab_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = iter(dataset)\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Functions transform the input sentence to a format that can be used for training\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Adding BOS, EOS and then make tensor\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# A function does everything and also padding\n",
    "def collate_fn(src, tgt):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in zip(src, tgt):\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Print an example\n",
    "batch_size = 2\n",
    "dataset = CustomDataset(train=True)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reprise de la session\\n', 'Resumption of the session\\n')\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "print(next(iter(dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Inputs: ('Il reste au demeurant bien du chemin à parcourir.\\n', \"Ce n' est pas possible.\\n\")\n",
      "('However, we still have a long way to go.\\n', 'It is quite out of the question.\\n')\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "fr_sentence, eng_sentence = next(iter(train_dataloader))\n",
    "print(f\"Raw Inputs: {fr_sentence}\\n{eng_sentence}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs: [['Il', 'reste', 'au', 'demeurant', 'bien', 'du', 'chemin', 'à', 'parcourir', '.'], ['Ce', 'n', \"'\", 'est', 'pas', 'possible', '.']]\n",
      "[['However', ',', 'we', 'still', 'have', 'a', 'long', 'way', 'to', 'go', '.'], ['It', 'is', 'quite', 'out', 'of', 'the', 'question', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Token transform\n",
    "fr_token, eng_token = [token_transform[\"fr\"](i.rstrip(\"\\n\")) for i in fr_sentence], [token_transform[\"en\"](i.rstrip(\"\\n\")) for i in eng_sentence]\n",
    "print(f\"Tokenized Inputs: {fr_token}\\n{eng_token}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Inputs to indicies: [[28, 259, 61, 5431, 90, 39, 596, 10, 3150, 5], [57, 116, 13, 6, 17, 413, 5]]\n",
      "[[158, 8, 28, 176, 29, 13, 242, 178, 9, 183, 5], [30, 6, 165, 169, 12, 7, 95, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Vocab transform\n",
    "fr_idx, eng_idx = [vocab_transform[\"fr\"](i) for i in fr_token], [vocab_transform[\"en\"](i) for i in eng_token]\n",
    "print(f\"Tokenized Inputs to indicies: {fr_idx}\\n{eng_idx}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Indicies with begin (2) and end token (3): [tensor([   2,   28,  259,   61, 5431,   90,   39,  596,   10, 3150,    5,    3]), tensor([  2,  57, 116,  13,   6,  17, 413,   5,   3])]\n",
      "[tensor([  2, 158,   8,  28, 176,  29,  13, 242, 178,   9, 183,   5,   3]), tensor([  2,  30,   6, 165, 169,  12,   7,  95,   5,   3])]\n"
     ]
    }
   ],
   "source": [
    "# Tensor transform\n",
    "fr_pad, eng_pad = [tensor_transform(i) for i in fr_idx], [tensor_transform(i) for i in eng_idx]\n",
    "print(f\"Tokenized Indicies with begin (2) and end token (3): {fr_pad}\\n{eng_pad}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding (1): tensor([[   2,    2],\n",
      "        [  28,   57],\n",
      "        [ 259,  116],\n",
      "        [  61,   13],\n",
      "        [5431,    6],\n",
      "        [  90,   17],\n",
      "        [  39,  413],\n",
      "        [ 596,    5],\n",
      "        [  10,    3],\n",
      "        [3150,    1],\n",
      "        [   5,    1],\n",
      "        [   3,    1]])\n",
      "tensor([[  2,   2],\n",
      "        [158,  30],\n",
      "        [  8,   6],\n",
      "        [ 28, 165],\n",
      "        [176, 169],\n",
      "        [ 29,  12],\n",
      "        [ 13,   7],\n",
      "        [242,  95],\n",
      "        [178,   5],\n",
      "        [  9,   3],\n",
      "        [183,   1],\n",
      "        [  5,   1],\n",
      "        [  3,   1]])\n"
     ]
    }
   ],
   "source": [
    "# # Lastly, we pad the rest of the sentence\n",
    "# This also changes the shape from (bs, seq_len) to (seq_len, bs)\n",
    "fr_pad, eng_pad = pad_sequence(fr_pad, padding_value=PAD_IDX), pad_sequence(eng_pad, padding_value=PAD_IDX)\n",
    "print(f\"After padding (1): {fr_pad}\\n{eng_pad}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Outputs: tensor([[   2,    2],\n",
      "        [  28,   57],\n",
      "        [ 259,  116],\n",
      "        [  61,   13],\n",
      "        [5431,    6],\n",
      "        [  90,   17],\n",
      "        [  39,  413],\n",
      "        [ 596,    5],\n",
      "        [  10,    3],\n",
      "        [3150,    1],\n",
      "        [   5,    1],\n",
      "        [   3,    1]])\n",
      "tensor([[  2,   2],\n",
      "        [158,  30],\n",
      "        [  8,   6],\n",
      "        [ 28, 165],\n",
      "        [176, 169],\n",
      "        [ 29,  12],\n",
      "        [ 13,   7],\n",
      "        [242,  95],\n",
      "        [178,   5],\n",
      "        [  9,   3],\n",
      "        [183,   1],\n",
      "        [  5,   1],\n",
      "        [  3,   1]])\n"
     ]
    }
   ],
   "source": [
    "# All the above is combined into collate_fn\n",
    "x, y = collate_fn(fr_sentence, eng_sentence)\n",
    "print(f\"Same Outputs: {x}\\n{y}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Luong Attention\n",
    "\n",
    "Implement GRU-based seq2seq model with Luong attention (https://arxiv.org/pdf/1508.04025.pdf) and train the model on the french to english translation dataset. (the use of the function nn.GRU() is allowed but the attention scheme needs to be implemented **explicitly**) :\n",
    "\n",
    "The Luong attention algorithm performs the following operations:\n",
    "\n",
    "1. The encoder generates a set of hidden states, $H = \\textbf{h}_i, i = 1, 2, .....T$ , from the input sentence. The decoder generates a set of hidden states, $S = \\textbf{s}_t, t =1, 2, .....$.\n",
    "2. The current decoder hidden state is computed as: $\\textbf{s}_t = GRU_{decoder}(\\textbf{s}_{t-1}, y_{t-1})$. Here, $\\textbf{s}_{t-1}$ denotes the previous hidden decoder state, and $y_{t-1}$ the current input, which is also the expected output for the previous timestep.\n",
    "\n",
    "3. A dot product on the encoder hidden state $\\textbf{h}_i$ and the current decoder hidden state $\\textbf{s}_t$ to compute the alignment scores: $e_{t,i} = \\textbf{s}_t . \\textbf{h}_i$. \n",
    "\n",
    "4. A softmax function is applied to the alignment scores, effectively normalizing them into attention weights in a range between 0 and 1: $\\alpha_{t, i} = \\text{softmax}(e_{t, i}/ \\textbf{e}_t)$.\n",
    "\n",
    "5. These attention weights together with the encoder hidden states are used to generate a context vector through a weighted sum: $\\textbf{c}_t = \\sum_{i=1}^T\\alpha_{t, i}\\textbf{h}_i$.\n",
    "\n",
    "6. An attentional hidden state is computed based on a weighted concatenation of the context vector and the current decoder hidden state: $\\tilde{\\textbf{s}_t} = \\text{tanh}\\big(W_c\\big[\\textbf{c}_t; \\textbf{s}_t\\big]\\big)$.\n",
    "\n",
    "7. The decoder produces a final output by feeding it a weighted attentional hidden state: $y_t = \\text{softmax}(W_y\\tilde{\\textbf{s}_t})$.\n",
    "\n",
    "8. Steps 2-7 are repeated until the end of the sequence. \n",
    "\n",
    "The attention has to be calculated in parallel via matrix multiplication. For loop $\\textbf{should not}$ be used.\n"
   ],
   "metadata": {
    "id": "2F_kTNpUg_tP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Fill UP ATTENTION NETWORK\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "\n",
    "        # for encoder\n",
    "        self.encoder_embedding = nn.Embedding(num_embeddings=SRC_VOCAB_SIZE, embedding_dim=hidden_size)\n",
    "        self.gru_encoder = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        # for decoder\n",
    "        self.decoder_embedding = nn.Embedding(num_embeddings=TGT_VOCAB_SIZE, embedding_dim=hidden_size)\n",
    "        self.gru_decoder = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        self.weighted_concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, TGT_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, h_init):\n",
    "        # x shape (seq_length, batch_size)\n",
    "        # y shape (seq_length, batch_size)\n",
    "        # h_init (1, batch_size, hidden_dim)\n",
    "\n",
    "        # encoder\n",
    "        # Shape (seq, batch, hidden_dim)\n",
    "        encoder_embedded = self.encoder_embedding(x)\n",
    "        # encoder_outputs (seq, batch_size, hidden_dim)\n",
    "        # encoder_hidden (batch_size, hidden_dim)\n",
    "        encoder_outputs, encoder_hidden = self.gru_encoder(encoder_embedded, h_init)\n",
    "\n",
    "        # decoder\n",
    "        # for now sequential:\n",
    "        # Shape (batch_size, hidden_dim)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # shape (seq_len, batch_size, hidden_dim)\n",
    "        decoder_embedded = self.decoder_embedding(y)\n",
    "        # decoder_outputs (seq, batch_size, hidden_dim)\n",
    "        # decoder_hidden (batch_size, hidden_dim)\n",
    "        decoder_outputs, decoder_hidden = self.gru_decoder(decoder_embedded, decoder_hidden)\n",
    "\n",
    "        # alignment score using dot product\n",
    "        # For each of the decoder hidden seq_out_[s], whose dimensions is [i], we dot product with\n",
    "        # every encoder hidden seq_in_[x], whose dimensions is [i], so that it is a scalar number for\n",
    "        # each [s,x]. Here we use einsum: sbi means the (seq_out_size, batch, dims),\n",
    "        # xbi means (seq_in_size, batch, dims), and we dot product of the last dimensions so in total\n",
    "        # the results is size_out, batch, size_in\n",
    "\n",
    "        # Shape (seq_out, batch, seq_in)\n",
    "        alignment_score = torch.einsum('sbi,xbi->sbx', [decoder_outputs, encoder_outputs])\n",
    "\n",
    "        #Shape (seq_out, batch, seq_in), if we sum over seq_in, we see probabilities = 1\n",
    "        # this mean for each seq_out, which token in seq_in to attend to\n",
    "        attention_weights = torch.nn.functional.softmax(alignment_score, dim=-1)\n",
    "\n",
    "        # The context is just the weighted of encoder_outputs by the attention_weights,\n",
    "        # attention_weights (seq_out, batch, seq_in), for each seq_out_[s] whose\n",
    "        # seq_in_[x] (a constant) contains probability, we multiply with encoder_outputs dimesions at each\n",
    "        # seq_in_[x] and summing together to acehive weighted encoder_outputs dim\n",
    "\n",
    "        # Shape (batch, seq_out, dim) # This calculates weights of each token in seq_out\n",
    "        context = torch.einsum('sbx,xbj->sbj', [attention_weights, encoder_outputs])\n",
    "\n",
    "        # Shape (seq_out, batch, dim*2)\n",
    "        concat_features = torch.cat([decoder_outputs, context], dim=-1)\n",
    "\n",
    "        # Shape (seq_out, batch, dim)\n",
    "        s_tilded = torch.tanh(self.weighted_concat(concat_features))\n",
    "\n",
    "        # Shape (seq_out, batch, VOCAB_SIZE)\n",
    "        output = self.out(s_tilded) # Swap batch size to first\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLuongAttention                           [15, 8, 6278]             --\n├─Embedding: 1-1                         [10, 8, 32]               249,504\n├─GRU: 1-2                               [10, 8, 32]               6,336\n├─Embedding: 1-3                         [15, 8, 32]               200,896\n├─GRU: 1-4                               [15, 8, 32]               6,336\n├─Linear: 1-5                            [15, 8, 32]               2,080\n├─Linear: 1-6                            [15, 8, 6278]             207,174\n==========================================================================================\nTotal params: 672,326\nTrainable params: 672,326\nNon-trainable params: 0\nTotal mult-adds (M): 9.91\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 6.16\nParams size (MB): 2.69\nEstimated Total Size (MB): 8.85\n=========================================================================================="
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use torchinfo to visualize the LuongAttention networks\n",
    "batch_size = 8\n",
    "seq_length_in = 10\n",
    "seq_length_out = 15\n",
    "hidden_dim = 32\n",
    "net = LuongAttention(hidden_dim)\n",
    "\n",
    "summary(net.to(device), input_size= [(seq_length_in, batch_size), (seq_length_out, batch_size),\n",
    "                          (1, batch_size, hidden_dim)],\n",
    "        dtypes=[torch.int, torch.int, torch.float])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "<graphviz.dot.Digraph at 0x7f234477a4f0>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: model Pages: 1 -->\n<svg width=\"544pt\" height=\"1537pt\"\n viewBox=\"0.00 0.00 544.00 1537.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1533)\">\n<title>model</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-1533 540,-1533 540,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_2</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"132,-1342 132,-1489 324,-1489 324,-1342 132,-1342\"/>\n<text text-anchor=\"middle\" x=\"174.5\" y=\"-1475.4\" font-family=\"Times,serif\" font-size=\"12.00\">Embedding</text>\n</g>\n<g id=\"clust2\" class=\"cluster\">\n<title>cluster_3</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"24,-1187 24,-1334 328,-1334 328,-1187 24,-1187\"/>\n<text text-anchor=\"middle\" x=\"46.5\" y=\"-1320.4\" font-family=\"Times,serif\" font-size=\"12.00\">GRU</text>\n</g>\n<g id=\"clust3\" class=\"cluster\">\n<title>cluster_4</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"336,-1187 336,-1334 528,-1334 528,-1187 336,-1187\"/>\n<text text-anchor=\"middle\" x=\"378.5\" y=\"-1320.4\" font-family=\"Times,serif\" font-size=\"12.00\">Embedding</text>\n</g>\n<g id=\"clust4\" class=\"cluster\">\n<title>cluster_5</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"150,-1032 150,-1179 454,-1179 454,-1032 150,-1032\"/>\n<text text-anchor=\"middle\" x=\"172.5\" y=\"-1165.4\" font-family=\"Times,serif\" font-size=\"12.00\">GRU</text>\n</g>\n<g id=\"clust5\" class=\"cluster\">\n<title>cluster_6</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"197,-301 197,-448 371,-448 371,-301 197,-301\"/>\n<text text-anchor=\"middle\" x=\"225\" y=\"-434.4\" font-family=\"Times,serif\" font-size=\"12.00\">Linear</text>\n</g>\n<g id=\"clust6\" class=\"cluster\">\n<title>cluster_7</title>\n<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"191,-8 191,-155 377,-155 377,-8 191,-8\"/>\n<text text-anchor=\"middle\" x=\"219\" y=\"-141.4\" font-family=\"Times,serif\" font-size=\"12.00\">Linear</text>\n</g>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"284,-1529 172,-1529 172,-1497 284,-1497 284,-1529\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"172,-1497 172,-1529 242,-1529 242,-1497 172,-1497\"/>\n<text text-anchor=\"start\" x=\"177\" y=\"-1516\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n<text text-anchor=\"start\" x=\"188.5\" y=\"-1505\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"242,-1497 242,-1529 284,-1529 284,-1497 242,-1497\"/>\n<text text-anchor=\"start\" x=\"247\" y=\"-1510.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8)</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"316,-1460 140,-1460 140,-1418 316,-1418 316,-1460\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"140,-1418 140,-1460 205,-1460 205,-1418 140,-1418\"/>\n<text text-anchor=\"start\" x=\"145\" y=\"-1442\" font-family=\"Linux libertine\" font-size=\"10.00\">embedding</text>\n<text text-anchor=\"start\" x=\"154\" y=\"-1431\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-1439 205,-1460 253,-1460 253,-1439 205,-1439\"/>\n<text text-anchor=\"start\" x=\"215\" y=\"-1447\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"253,-1439 253,-1460 316,-1460 316,-1439 253,-1439\"/>\n<text text-anchor=\"start\" x=\"267\" y=\"-1447\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-1418 205,-1439 253,-1439 253,-1418 205,-1418\"/>\n<text text-anchor=\"start\" x=\"210\" y=\"-1426\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"253,-1418 253,-1439 316,-1439 316,-1418 253,-1418\"/>\n<text text-anchor=\"start\" x=\"258\" y=\"-1426\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8, 32) </text>\n</g>\n<!-- 0&#45;&gt;3 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M228,-1496.73C228,-1489.08 228,-1479.52 228,-1470.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"231.5,-1470.17 228,-1460.17 224.5,-1470.17 231.5,-1470.17\"/>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"488,-1382 376,-1382 376,-1350 488,-1350 488,-1382\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"376,-1350 376,-1382 446,-1382 446,-1350 376,-1350\"/>\n<text text-anchor=\"start\" x=\"381\" y=\"-1369\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n<text text-anchor=\"start\" x=\"392.5\" y=\"-1358\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"446,-1350 446,-1382 488,-1382 488,-1350 446,-1350\"/>\n<text text-anchor=\"start\" x=\"451\" y=\"-1363.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8)</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"520,-1305 344,-1305 344,-1263 520,-1263 520,-1305\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"344,-1263 344,-1305 409,-1305 409,-1263 344,-1263\"/>\n<text text-anchor=\"start\" x=\"349\" y=\"-1287\" font-family=\"Linux libertine\" font-size=\"10.00\">embedding</text>\n<text text-anchor=\"start\" x=\"358\" y=\"-1276\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"409,-1284 409,-1305 457,-1305 457,-1284 409,-1284\"/>\n<text text-anchor=\"start\" x=\"419\" y=\"-1292\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"457,-1284 457,-1305 520,-1305 520,-1284 457,-1284\"/>\n<text text-anchor=\"start\" x=\"471\" y=\"-1292\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"409,-1263 409,-1284 457,-1284 457,-1263 409,-1263\"/>\n<text text-anchor=\"start\" x=\"414\" y=\"-1271\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"457,-1263 457,-1284 520,-1284 520,-1263 457,-1263\"/>\n<text text-anchor=\"start\" x=\"462\" y=\"-1271\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32) </text>\n</g>\n<!-- 1&#45;&gt;8 -->\n<g id=\"edge9\" class=\"edge\">\n<title>1&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M432,-1349.95C432,-1340.15 432,-1327.02 432,-1315.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"435.5,-1315.12 432,-1305.12 428.5,-1315.12 435.5,-1315.12\"/>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"124,-1382 0,-1382 0,-1350 124,-1350 124,-1382\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-1350 0,-1382 70,-1382 70,-1350 0,-1350\"/>\n<text text-anchor=\"start\" x=\"5\" y=\"-1369\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n<text text-anchor=\"start\" x=\"16.5\" y=\"-1358\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"70,-1350 70,-1382 124,-1382 124,-1350 70,-1350\"/>\n<text text-anchor=\"start\" x=\"75\" y=\"-1363.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 8, 32)</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"280,-1305 72,-1305 72,-1263 280,-1263 280,-1305\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"72,-1263 72,-1305 119,-1305 119,-1263 72,-1263\"/>\n<text text-anchor=\"start\" x=\"87.5\" y=\"-1287\" font-family=\"Linux libertine\" font-size=\"10.00\">gru</text>\n<text text-anchor=\"start\" x=\"77\" y=\"-1276\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"119,-1284 119,-1305 167,-1305 167,-1284 119,-1284\"/>\n<text text-anchor=\"start\" x=\"129\" y=\"-1292\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"167,-1284 167,-1305 280,-1305 280,-1284 167,-1284\"/>\n<text text-anchor=\"start\" x=\"172\" y=\"-1292\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8, 32), (1, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"119,-1263 119,-1284 167,-1284 167,-1263 119,-1263\"/>\n<text text-anchor=\"start\" x=\"124\" y=\"-1271\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"167,-1263 167,-1284 280,-1284 280,-1263 167,-1263\"/>\n<text text-anchor=\"start\" x=\"172\" y=\"-1271\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8, 32), (1, 8, 32) </text>\n</g>\n<!-- 2&#45;&gt;5 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M83.44,-1349.95C99.1,-1338.96 120.73,-1323.79 139.14,-1310.87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"141.16,-1313.73 147.33,-1305.12 137.14,-1308 141.16,-1313.73\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"288,-1382 150,-1382 150,-1350 288,-1350 288,-1382\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"150,-1350 150,-1382 228,-1382 228,-1350 150,-1350\"/>\n<text text-anchor=\"start\" x=\"155\" y=\"-1369\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"170.5\" y=\"-1358\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"228,-1350 228,-1382 288,-1382 288,-1350 228,-1350\"/>\n<text text-anchor=\"start\" x=\"233\" y=\"-1363.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8, 32)</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge2\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.45,-1417.84C224.44,-1409.89 223.27,-1400.66 222.2,-1392.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"225.66,-1391.72 220.93,-1382.24 218.72,-1392.6 225.66,-1391.72\"/>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M210.91,-1349.95C205.48,-1339.85 198.16,-1326.22 191.62,-1314.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.63,-1312.27 186.81,-1305.12 188.46,-1315.58 194.63,-1312.27\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"170,-1227 32,-1227 32,-1195 170,-1195 170,-1227\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"32,-1195 32,-1227 110,-1227 110,-1195 32,-1195\"/>\n<text text-anchor=\"start\" x=\"37\" y=\"-1214\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"52.5\" y=\"-1203\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"110,-1195 110,-1227 170,-1227 170,-1195 110,-1195\"/>\n<text text-anchor=\"start\" x=\"115\" y=\"-1208.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(10, 8, 32)</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M154.71,-1262.84C145.28,-1253.92 134.15,-1243.38 124.48,-1234.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"126.76,-1231.57 117.1,-1227.24 121.95,-1236.65 126.76,-1231.57\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"320,-1227 188,-1227 188,-1195 320,-1195 320,-1227\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"188,-1195 188,-1227 266,-1227 266,-1195 188,-1195\"/>\n<text text-anchor=\"start\" x=\"193\" y=\"-1214\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"208.5\" y=\"-1203\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"266,-1195 266,-1227 320,-1227 320,-1195 266,-1195\"/>\n<text text-anchor=\"start\" x=\"271\" y=\"-1208.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 8, 32)</text>\n</g>\n<!-- 5&#45;&gt;7 -->\n<g id=\"edge8\" class=\"edge\">\n<title>5&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M198.14,-1262.84C207.95,-1253.92 219.52,-1243.38 229.58,-1234.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"232.22,-1236.56 237.26,-1227.24 227.51,-1231.38 232.22,-1236.56\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"324,-1004 110,-1004 110,-962 324,-962 324,-1004\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"110,-962 110,-1004 157,-1004 157,-962 110,-962\"/>\n<text text-anchor=\"start\" x=\"115\" y=\"-986\" font-family=\"Linux libertine\" font-size=\"10.00\">einsum</text>\n<text text-anchor=\"start\" x=\"115\" y=\"-975\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"157,-983 157,-1004 205,-1004 205,-983 157,-983\"/>\n<text text-anchor=\"start\" x=\"167\" y=\"-991\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-983 205,-1004 324,-1004 324,-983 205,-983\"/>\n<text text-anchor=\"start\" x=\"210\" y=\"-991\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32), (10, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"157,-962 157,-983 205,-983 205,-962 157,-962\"/>\n<text text-anchor=\"start\" x=\"162\" y=\"-970\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-962 205,-983 324,-983 324,-962 205,-962\"/>\n<text text-anchor=\"start\" x=\"238\" y=\"-970\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 10) </text>\n</g>\n<!-- 6&#45;&gt;13 -->\n<g id=\"edge5\" class=\"edge\">\n<title>6&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M101.3,-1194.68C102.65,-1161.89 109.74,-1083.66 146,-1032 151.9,-1023.6 159.77,-1016.24 168.13,-1009.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"170.4,-1012.65 176.6,-1004.06 166.4,-1006.91 170.4,-1012.65\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"324,-712 110,-712 110,-670 324,-670 324,-712\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"110,-670 110,-712 157,-712 157,-670 110,-670\"/>\n<text text-anchor=\"start\" x=\"115\" y=\"-694\" font-family=\"Linux libertine\" font-size=\"10.00\">einsum</text>\n<text text-anchor=\"start\" x=\"115\" y=\"-683\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"157,-691 157,-712 205,-712 205,-691 157,-691\"/>\n<text text-anchor=\"start\" x=\"167\" y=\"-699\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-691 205,-712 324,-712 324,-691 205,-691\"/>\n<text text-anchor=\"start\" x=\"210\" y=\"-699\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 10), (10, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"157,-670 157,-691 205,-691 205,-670 157,-670\"/>\n<text text-anchor=\"start\" x=\"162\" y=\"-678\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-670 205,-691 324,-691 324,-670 205,-670\"/>\n<text text-anchor=\"start\" x=\"238\" y=\"-678\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32) </text>\n</g>\n<!-- 6&#45;&gt;17 -->\n<g id=\"edge6\" class=\"edge\">\n<title>6&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M97.86,-1194.88C92.47,-1167.5 82,-1107.86 82,-1057 82,-1057 82,-1057 82,-836 82,-783.27 130.77,-741.91 169.68,-717.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"171.62,-720.21 178.31,-712 167.96,-714.25 171.62,-720.21\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"392,-1150 184,-1150 184,-1108 392,-1108 392,-1150\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"184,-1108 184,-1150 231,-1150 231,-1108 184,-1108\"/>\n<text text-anchor=\"start\" x=\"199.5\" y=\"-1132\" font-family=\"Linux libertine\" font-size=\"10.00\">gru</text>\n<text text-anchor=\"start\" x=\"189\" y=\"-1121\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"231,-1129 231,-1150 279,-1150 279,-1129 231,-1129\"/>\n<text text-anchor=\"start\" x=\"241\" y=\"-1137\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"279,-1129 279,-1150 392,-1150 392,-1129 279,-1129\"/>\n<text text-anchor=\"start\" x=\"284\" y=\"-1137\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32), (1, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"231,-1108 231,-1129 279,-1129 279,-1108 231,-1108\"/>\n<text text-anchor=\"start\" x=\"236\" y=\"-1116\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"279,-1108 279,-1129 392,-1129 392,-1108 279,-1108\"/>\n<text text-anchor=\"start\" x=\"284\" y=\"-1116\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32), (1, 8, 32) </text>\n</g>\n<!-- 7&#45;&gt;10 -->\n<g id=\"edge12\" class=\"edge\">\n<title>7&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M260.4,-1194.95C264.65,-1184.95 270.37,-1171.49 275.5,-1159.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"278.76,-1160.69 279.45,-1150.12 272.32,-1157.95 278.76,-1160.69\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"491,-1227 353,-1227 353,-1195 491,-1195 491,-1227\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"353,-1195 353,-1227 431,-1227 431,-1195 353,-1195\"/>\n<text text-anchor=\"start\" x=\"358\" y=\"-1214\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"373.5\" y=\"-1203\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"431,-1195 431,-1227 491,-1227 491,-1195 431,-1195\"/>\n<text text-anchor=\"start\" x=\"436\" y=\"-1208.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32)</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge10\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M429.16,-1262.84C428.04,-1254.89 426.74,-1245.66 425.56,-1237.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"429.01,-1236.65 424.15,-1227.24 422.07,-1237.63 429.01,-1236.65\"/>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M396.8,-1194.95C378.14,-1183.81 352.28,-1168.38 330.46,-1155.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"332.08,-1152.24 321.69,-1150.12 328.49,-1158.25 332.08,-1152.24\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"296,-1072 158,-1072 158,-1040 296,-1040 296,-1072\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"158,-1040 158,-1072 236,-1072 236,-1040 158,-1040\"/>\n<text text-anchor=\"start\" x=\"163\" y=\"-1059\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"178.5\" y=\"-1048\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"236,-1040 236,-1072 296,-1072 296,-1040 236,-1040\"/>\n<text text-anchor=\"start\" x=\"241\" y=\"-1053.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32)</text>\n</g>\n<!-- 10&#45;&gt;11 -->\n<g id=\"edge15\" class=\"edge\">\n<title>10&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M270.68,-1107.84C263.24,-1099.18 254.5,-1089.01 246.8,-1080.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"249.26,-1077.54 240.09,-1072.24 243.95,-1082.1 249.26,-1077.54\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"446,-1072 314,-1072 314,-1040 446,-1040 446,-1072\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"314,-1040 314,-1072 392,-1072 392,-1040 314,-1040\"/>\n<text text-anchor=\"start\" x=\"319\" y=\"-1059\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"334.5\" y=\"-1048\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"392,-1040 392,-1072 446,-1072 446,-1040 392,-1040\"/>\n<text text-anchor=\"start\" x=\"397\" y=\"-1053.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 8, 32)</text>\n</g>\n<!-- 10&#45;&gt;12 -->\n<g id=\"edge16\" class=\"edge\">\n<title>10&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314.12,-1107.84C326.12,-1098.58 340.35,-1087.6 352.51,-1078.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"354.77,-1080.89 360.55,-1072.01 350.49,-1075.35 354.77,-1080.89\"/>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M224.88,-1039.94C223.82,-1032.45 222.51,-1023.12 221.26,-1014.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"224.7,-1013.57 219.84,-1004.16 217.77,-1014.55 224.7,-1013.57\"/>\n</g>\n<!-- 19 -->\n<g id=\"node20\" class=\"node\">\n<title>19</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"372,-566 196,-566 196,-524 372,-524 372,-566\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"196,-524 196,-566 243,-566 243,-524 196,-524\"/>\n<text text-anchor=\"start\" x=\"211.5\" y=\"-548\" font-family=\"Linux libertine\" font-size=\"10.00\">cat</text>\n<text text-anchor=\"start\" x=\"201\" y=\"-537\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"243,-545 243,-566 291,-566 291,-545 243,-545\"/>\n<text text-anchor=\"start\" x=\"253\" y=\"-553\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"291,-545 291,-566 372,-566 372,-545 291,-545\"/>\n<text text-anchor=\"start\" x=\"296\" y=\"-553\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (15, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"243,-524 243,-545 291,-545 291,-524 243,-524\"/>\n<text text-anchor=\"start\" x=\"248\" y=\"-532\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"291,-524 291,-545 372,-545 372,-524 291,-524\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-532\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 64) </text>\n</g>\n<!-- 11&#45;&gt;19 -->\n<g id=\"edge14\" class=\"edge\">\n<title>11&#45;&gt;19</title>\n<path fill=\"none\" stroke=\"black\" d=\"M281.48,-1039.9C300.39,-1032.09 320.07,-1020.59 333,-1004 358.93,-970.72 352,-953.19 352,-911 352,-911 352,-911 352,-690 352,-646.4 326.2,-601.88 306.38,-574.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"309.2,-572.04 300.46,-566.05 303.56,-576.18 309.2,-572.04\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"286,-926 148,-926 148,-894 286,-894 286,-926\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"148,-894 148,-926 226,-926 226,-894 148,-894\"/>\n<text text-anchor=\"start\" x=\"153\" y=\"-913\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"168.5\" y=\"-902\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"226,-894 226,-926 286,-926 286,-894 226,-894\"/>\n<text text-anchor=\"start\" x=\"231\" y=\"-907.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 10)</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge18\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M217,-961.84C217,-953.89 217,-944.66 217,-936.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"220.5,-936.24 217,-926.24 213.5,-936.24 220.5,-936.24\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"297.5,-858 136.5,-858 136.5,-816 297.5,-816 297.5,-858\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"137,-816 137,-858 187,-858 187,-816 137,-816\"/>\n<text text-anchor=\"start\" x=\"142\" y=\"-840\" font-family=\"Linux libertine\" font-size=\"10.00\">softmax</text>\n<text text-anchor=\"start\" x=\"143.5\" y=\"-829\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"187,-837 187,-858 235,-858 235,-837 187,-837\"/>\n<text text-anchor=\"start\" x=\"197\" y=\"-845\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"235,-837 235,-858 298,-858 298,-837 235,-837\"/>\n<text text-anchor=\"start\" x=\"240\" y=\"-845\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 10) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"187,-816 187,-837 235,-837 235,-816 187,-816\"/>\n<text text-anchor=\"start\" x=\"192\" y=\"-824\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"235,-816 235,-837 298,-837 298,-816 235,-816\"/>\n<text text-anchor=\"start\" x=\"240\" y=\"-824\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 10) </text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge17\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M217,-893.94C217,-886.45 217,-877.12 217,-868.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"220.5,-868.16 217,-858.16 213.5,-868.16 220.5,-868.16\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"286,-780 148,-780 148,-748 286,-748 286,-780\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"148,-748 148,-780 226,-780 226,-748 148,-748\"/>\n<text text-anchor=\"start\" x=\"153\" y=\"-767\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"168.5\" y=\"-756\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"226,-748 226,-780 286,-780 286,-748 226,-748\"/>\n<text text-anchor=\"start\" x=\"231\" y=\"-761.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 10)</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge20\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M217,-815.84C217,-807.89 217,-798.66 217,-790.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"220.5,-790.24 217,-780.24 213.5,-790.24 220.5,-790.24\"/>\n</g>\n<!-- 16&#45;&gt;17 -->\n<g id=\"edge19\" class=\"edge\">\n<title>16&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M217,-747.94C217,-740.45 217,-731.12 217,-722.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"220.5,-722.16 217,-712.16 213.5,-722.16 220.5,-722.16\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"305,-634 167,-634 167,-602 305,-602 305,-634\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"167,-602 167,-634 245,-634 245,-602 167,-602\"/>\n<text text-anchor=\"start\" x=\"172\" y=\"-621\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"187.5\" y=\"-610\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"245,-602 245,-634 305,-634 305,-602 245,-602\"/>\n<text text-anchor=\"start\" x=\"250\" y=\"-615.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32)</text>\n</g>\n<!-- 17&#45;&gt;18 -->\n<g id=\"edge22\" class=\"edge\">\n<title>17&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M222.39,-669.84C224.55,-661.8 227.05,-652.45 229.31,-643.98\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"232.72,-644.8 231.92,-634.24 225.96,-642.99 232.72,-644.8\"/>\n</g>\n<!-- 18&#45;&gt;19 -->\n<g id=\"edge21\" class=\"edge\">\n<title>18&#45;&gt;19</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.18,-601.94C251.57,-593.96 258.38,-583.9 264.73,-574.51\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"267.67,-576.4 270.37,-566.16 261.87,-572.48 267.67,-576.4\"/>\n</g>\n<!-- 20 -->\n<g id=\"node21\" class=\"node\">\n<title>20</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"353,-488 215,-488 215,-456 353,-456 353,-488\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"215,-456 215,-488 293,-488 293,-456 215,-456\"/>\n<text text-anchor=\"start\" x=\"220\" y=\"-475\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-464\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"293,-456 293,-488 353,-488 353,-456 293,-456\"/>\n<text text-anchor=\"start\" x=\"298\" y=\"-469.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 64)</text>\n</g>\n<!-- 19&#45;&gt;20 -->\n<g id=\"edge23\" class=\"edge\">\n<title>19&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-523.84C284,-515.89 284,-506.66 284,-498.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-498.24 284,-488.24 280.5,-498.24 287.5,-498.24\"/>\n</g>\n<!-- 21 -->\n<g id=\"node22\" class=\"node\">\n<title>21</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"363,-419 205,-419 205,-377 363,-377 363,-419\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-377 205,-419 252,-419 252,-377 205,-377\"/>\n<text text-anchor=\"start\" x=\"214.5\" y=\"-401\" font-family=\"Linux libertine\" font-size=\"10.00\">linear</text>\n<text text-anchor=\"start\" x=\"210\" y=\"-390\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"252,-398 252,-419 300,-419 300,-398 252,-398\"/>\n<text text-anchor=\"start\" x=\"262\" y=\"-406\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"300,-398 300,-419 363,-419 363,-398 300,-398\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-406\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 64) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"252,-377 252,-398 300,-398 300,-377 252,-377\"/>\n<text text-anchor=\"start\" x=\"257\" y=\"-385\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"300,-377 300,-398 363,-398 363,-377 300,-377\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-385\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32) </text>\n</g>\n<!-- 20&#45;&gt;21 -->\n<g id=\"edge24\" class=\"edge\">\n<title>20&#45;&gt;21</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-455.73C284,-448.08 284,-438.52 284,-429.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-429.17 284,-419.17 280.5,-429.17 287.5,-429.17\"/>\n</g>\n<!-- 22 -->\n<g id=\"node23\" class=\"node\">\n<title>22</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"353,-341 215,-341 215,-309 353,-309 353,-341\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"215,-309 215,-341 293,-341 293,-309 215,-309\"/>\n<text text-anchor=\"start\" x=\"220\" y=\"-328\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-317\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"293,-309 293,-341 353,-341 353,-309 293,-309\"/>\n<text text-anchor=\"start\" x=\"298\" y=\"-322.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32)</text>\n</g>\n<!-- 21&#45;&gt;22 -->\n<g id=\"edge26\" class=\"edge\">\n<title>21&#45;&gt;22</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-376.84C284,-368.89 284,-359.66 284,-351.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-351.24 284,-341.24 280.5,-351.24 287.5,-351.24\"/>\n</g>\n<!-- 23 -->\n<g id=\"node24\" class=\"node\">\n<title>23</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"363,-273 205,-273 205,-231 363,-231 363,-273\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"205,-231 205,-273 252,-273 252,-231 205,-231\"/>\n<text text-anchor=\"start\" x=\"217.5\" y=\"-255\" font-family=\"Linux libertine\" font-size=\"10.00\">tanh</text>\n<text text-anchor=\"start\" x=\"210\" y=\"-244\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"252,-252 252,-273 300,-273 300,-252 252,-252\"/>\n<text text-anchor=\"start\" x=\"262\" y=\"-260\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"300,-252 300,-273 363,-273 363,-252 300,-252\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-260\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"252,-231 252,-252 300,-252 300,-231 252,-231\"/>\n<text text-anchor=\"start\" x=\"257\" y=\"-239\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"300,-231 300,-252 363,-252 363,-231 300,-231\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-239\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32) </text>\n</g>\n<!-- 22&#45;&gt;23 -->\n<g id=\"edge25\" class=\"edge\">\n<title>22&#45;&gt;23</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-308.94C284,-301.45 284,-292.12 284,-283.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-283.16 284,-273.16 280.5,-283.16 287.5,-283.16\"/>\n</g>\n<!-- 24 -->\n<g id=\"node25\" class=\"node\">\n<title>24</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"353,-195 215,-195 215,-163 353,-163 353,-195\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"215,-163 215,-195 293,-195 293,-163 215,-163\"/>\n<text text-anchor=\"start\" x=\"220\" y=\"-182\" font-family=\"Linux libertine\" font-size=\"10.00\">hidden&#45;tensor</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-171\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"293,-163 293,-195 353,-195 353,-163 293,-163\"/>\n<text text-anchor=\"start\" x=\"298\" y=\"-176.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32)</text>\n</g>\n<!-- 23&#45;&gt;24 -->\n<g id=\"edge27\" class=\"edge\">\n<title>23&#45;&gt;24</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-230.84C284,-222.89 284,-213.66 284,-205.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-205.24 284,-195.24 280.5,-205.24 287.5,-205.24\"/>\n</g>\n<!-- 25 -->\n<g id=\"node26\" class=\"node\">\n<title>25</title>\n<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"369,-126 199,-126 199,-84 369,-84 369,-126\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"199,-84 199,-126 246,-126 246,-84 199,-84\"/>\n<text text-anchor=\"start\" x=\"208.5\" y=\"-108\" font-family=\"Linux libertine\" font-size=\"10.00\">linear</text>\n<text text-anchor=\"start\" x=\"204\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"246,-105 246,-126 294,-126 294,-105 246,-105\"/>\n<text text-anchor=\"start\" x=\"256\" y=\"-113\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"294,-105 294,-126 369,-126 369,-105 294,-105\"/>\n<text text-anchor=\"start\" x=\"305\" y=\"-113\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 32) </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"246,-84 246,-105 294,-105 294,-84 246,-84\"/>\n<text text-anchor=\"start\" x=\"251\" y=\"-92\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n<polygon fill=\"none\" stroke=\"black\" points=\"294,-84 294,-105 369,-105 369,-84 294,-84\"/>\n<text text-anchor=\"start\" x=\"299\" y=\"-92\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 6278) </text>\n</g>\n<!-- 24&#45;&gt;25 -->\n<g id=\"edge28\" class=\"edge\">\n<title>24&#45;&gt;25</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-162.73C284,-155.08 284,-145.52 284,-136.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-136.17 284,-126.17 280.5,-136.17 287.5,-136.17\"/>\n</g>\n<!-- 26 -->\n<g id=\"node27\" class=\"node\">\n<title>26</title>\n<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"358.5,-48 209.5,-48 209.5,-16 358.5,-16 358.5,-48\"/>\n<polygon fill=\"none\" stroke=\"black\" points=\"210,-16 210,-48 287,-48 287,-16 210,-16\"/>\n<text text-anchor=\"start\" x=\"215\" y=\"-35\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n<text text-anchor=\"start\" x=\"230\" y=\"-24\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n<polygon fill=\"none\" stroke=\"black\" points=\"287,-16 287,-48 359,-48 359,-16 287,-16\"/>\n<text text-anchor=\"start\" x=\"292\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(15, 8, 6278)</text>\n</g>\n<!-- 25&#45;&gt;26 -->\n<g id=\"edge29\" class=\"edge\">\n<title>25&#45;&gt;26</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-83.84C284,-75.89 284,-66.66 284,-58.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-58.24 284,-48.24 280.5,-58.24 287.5,-58.24\"/>\n</g>\n</g>\n</svg>\n"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_graph = draw_graph(net.to(device),\n",
    "                         input_size= [(seq_length_in, batch_size), (seq_length_out, batch_size),\n",
    "                          (1, batch_size, hidden_dim)],\n",
    "        dtypes=[torch.int, torch.int, torch.float], roll=True, expand_nested=True,\n",
    "                         hide_module_functions=False, hide_inner_tensors=False)\n",
    "model_graph.visual_graph\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "'./temp_roll.pdf'"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_graph.visual_graph.render('temp_roll')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "hidden_size = 256\n",
    "my_lr = 1.3\n",
    "bs = 32\n",
    "\n",
    "# Variables\n",
    "train_dataset = CustomDataset(train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = CustomDataset(train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "net = LuongAttention(hidden_size)\n",
    "\n",
    "print(net)\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
   ],
   "metadata": {
    "id": "61CLe7y-x9P7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3095678a-2ab5-4da1-f601-4bfc00296d30",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LuongAttention(\n",
      "  (encoder_embedding): Embedding(7797, 256)\n",
      "  (gru_encoder): GRU(256, 256)\n",
      "  (decoder_embedding): Embedding(6278, 256)\n",
      "  (gru_decoder): GRU(256, 256)\n",
      "  (weighted_concat): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=6278, bias=True)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    h=h.to(device)\n",
    "\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = collate_fn(x, y)\n",
    "        # Batch size might be different for the last batch\n",
    "        batch_size = x.size()[1]\n",
    "        seq_length = y.size()[0] - 1\n",
    "        # set the initial h to be the zero vector\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        # send them to the gpu\n",
    "        minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "        minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "        h=h.to(device)\n",
    "\n",
    "        # FILL UP FORWARD PASS\n",
    "        scores = net(minibatch_data, minibatch_label, h)\n",
    "        #minibatch_label = torch.swapaxes(minibatch_label, 0, 1)\n",
    "        #scores = scores[:, :-1, :] #\n",
    "        minibatch_label = minibatch_label[:, 1:]\n",
    "        loss = criterion(torch.swapaxes(scores, 1, 2), minibatch_label)\n",
    "        # END OF FORWARD PASS\n",
    "\n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # update the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        # Collect garbage to prevent OOM\n",
    "        gc.collect()\n",
    "\n",
    "    total_loss = running_loss/num_batches\n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    return math.exp(total_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:\n",
    "        for p in net.parameters():\n",
    "            p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 52.48261547088623 \t lr= 1.3 \t exp(loss)= 128.35715908107403\n",
      "\n",
      "epoch= 1 \t time= 105.0176751613617 \t lr= 1.3 \t exp(loss)= 40.80311326012056\n",
      "\n",
      "epoch= 2 \t time= 157.5245852470398 \t lr= 1.3 \t exp(loss)= 24.970665743638587\n",
      "\n",
      "epoch= 3 \t time= 210.24461817741394 \t lr= 1.3 \t exp(loss)= 17.483279475864972\n",
      "\n",
      "epoch= 4 \t time= 263.1889922618866 \t lr= 1.1818181818181817 \t exp(loss)= 12.57862863031868\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "def train(my_lr):\n",
    "    for epoch in range(num_epochs):\n",
    "      # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "        if epoch >= 4:\n",
    "            my_lr = my_lr / 1.1\n",
    "\n",
    "        # create a new optimizer and give the current learning rate.\n",
    "        optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "\n",
    "        # set the running quantities to zero at the beginning of the epoch\n",
    "        running_loss=0\n",
    "        num_batches=0\n",
    "\n",
    "        for x, y in train_dataloader:\n",
    "            # Set the gradients to zeros\n",
    "            optimizer.zero_grad()\n",
    "            # Transform inputs\n",
    "            x, y = collate_fn(x, y)\n",
    "            # Batch size might be different for the last batch\n",
    "            batch_size = x.size()[1]\n",
    "            seq_length = y.size()[0] - 1\n",
    "            # set the initial h to be the zero vector\n",
    "            h = torch.zeros(1, batch_size, hidden_size)\n",
    "            # send them to the gpu\n",
    "            minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "            minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "            h=h.to(device)\n",
    "\n",
    "            # FILL UP FORWARD PASS\n",
    "            scores = net(minibatch_data, minibatch_label, h) # Output is (seq, bs, dims)\n",
    "            # Label must have shape (batch_size, seq)\n",
    "            minibatch_label = torch.einsum('sb->bs', [minibatch_label])[:, 1:]\n",
    "            # Scores must have shape (batch, dims, seq)\n",
    "            # In torch document, input shape (N,C,d_1,...,d_k), where C is class which means\n",
    "            # dims is class because it outputs the probabilities of vocab size\n",
    "            scores = torch.einsum('sbd->bds', [scores])[:, :, :-1]\n",
    "            loss = criterion(scores, minibatch_label)\n",
    "            # END OF FORWARD PASS\n",
    "\n",
    "            # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "            loss.backward()\n",
    "\n",
    "            # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "            normalize_gradient(net)\n",
    "            optimizer.step()\n",
    "            # update the running loss\n",
    "            running_loss += loss.detach().item()\n",
    "            num_batches += 1\n",
    "            # Collect garbage to prevent OOM\n",
    "            gc.collect()\n",
    "        # compute stats for the full training set\n",
    "        total_loss = running_loss / num_batches\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print('')\n",
    "        print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "        #eval_on_test_set()\n",
    "\n",
    "train(my_lr=my_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "count = 0\n",
    "for x, y in test_dataloader:\n",
    "    print(x)\n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    x, y = collate_fn(x, y)\n",
    "    # send them to the gpu\n",
    "    minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "    # The first prediction is the start of sentence index\n",
    "    start_index = torch.tensor([[2]]).type(torch.LongTensor).to(device)\n",
    "    predictions=start_index\n",
    "    for _ in range(20):\n",
    "      # At every loop, pass in the previous predictions\n",
    "      predictions = net.forward(minibatch_data, predictions, h)\n",
    "      predictions = torch.reshape(predictions, (-1, TGT_VOCAB_SIZE, 1))\n",
    "      # Get the new predictions shifted right by 1 timestep\n",
    "      predictions = torch.argmax(predictions, dim=1)\n",
    "      # Add back the first timestep\n",
    "      predictions = torch.cat([start_index, predictions], 0)\n",
    "      if predictions[-1].item() == 3:\n",
    "          break\n",
    "    predictions = predictions.reshape(-1)\n",
    "    # Transform from token to words\n",
    "    predictions = [vocab_transform[TGT_LANGUAGE].lookup_token(i) for i in predictions]\n",
    "    print(f\"Label: {[vocab_transform[TGT_LANGUAGE].lookup_token(i) for i in y]}\")\n",
    "    print(f\"Predicted: {predictions}\")\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ],
   "metadata": {
    "id": "RijG2UdmSTdT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b1f1d22-62bb-46fe-fa45-7dc7769a7928",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mais qui a peur ?\\n',)\n",
      "Label: ['<bos>', 'But', 'who', 'fears', ',', 'exactly', '?', '<eos>']\n",
      "Predicted: ['<bos>', 'Why', 'are', 'we', 'to', 'be', 'done', '?', '<eos>']\n",
      "('Sapristi, mais quelle Europe est-ce donc ? !\\n',)\n",
      "Label: ['<bos>', 'For', '<unk>', \"'s\", 'sake', '!', 'What', 'sort', 'of', 'Europe', 'is', 'this', '?', '<eos>']\n",
      "Predicted: ['<bos>', 'What', 'is', 'this', 'be', 'approved', '!', '<eos>']\n",
      "('Pourquoi cette omission ?\\n',)\n",
      "Label: ['<bos>', 'Why', 'is', 'this', '?', '<eos>']\n",
      "Predicted: ['<bos>', 'Why', '?', '<eos>']\n",
      "('Ce sera ma première question.\\n',)\n",
      "Label: ['<bos>', 'That', 'is', 'my', 'first', 'question', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'It', 'is', 'a', 'question', '.', '<eos>']\n",
      "(\"Nous sommes d'accord !\\n\",)\n",
      "Label: ['<bos>', 'We', 'agree', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'We', 'are', 'all', 'for', 'this', '!', '<eos>']\n",
      "(\"Je suis encore bouleversé et choqué par ce que j'ai vu.\\n\",)\n",
      "Label: ['<bos>', 'I', 'am', 'still', '<unk>', 'from', 'the', 'shock', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'I', 'am', 'sure', 'that', 'this', 'is', 'no', 'to', 'be', 'and', 'I', 'have', 'been', 'doing', 'so', '.', '<eos>']\n",
      "(\"Heureusement, personne n'a été blessé.\\n\",)\n",
      "Label: ['<bos>', 'Fortunately', 'no', 'one', 'was', '<unk>', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'It', 'is', 'not', 'been', 'a', 'good', 'thing', '.', '<eos>']\n",
      "('Peut-être comprendront-ils mieux.\\n',)\n",
      "Label: ['<bos>', 'Perhaps', 'they', 'would', 'come', 'to', 'understand', 'better', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'It', 'is', 'now', ',', 'however', ',', 'must', 'be', 'clear', '.', '<eos>']\n",
      "(\"Les Européens n'en ont aucune idée.\\n\",)\n",
      "Label: ['<bos>', 'People', 'of', 'Europe', 'have', 'no', 'idea', 'of', 'this', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'It', 'is', 'not', 'true', '.', '<eos>']\n",
      "('Je donne entièrement raison au Commissaire Patten.\\n',)\n",
      "Label: ['<bos>', 'I', 'completely', 'agree', 'with', 'Commissioner', 'Patten', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'I', 'would', 'like', 'to', 'be', 'a', 'point', 'of', 'order', '.', '<eos>']\n",
      "('Il ne peut en être ainsi !\\n',)\n",
      "Label: ['<bos>', 'This', 'is', 'completely', 'unacceptable', '.', '<eos>']\n",
      "Predicted: ['<bos>', 'It', 'is', 'not', 'the', 'case', '!', '<eos>']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MWjgSSE1wBOf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}